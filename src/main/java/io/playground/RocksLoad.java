/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package io.playground;

import com.google.common.primitives.Longs;
import java.util.List;
import java.util.Random;
import java.util.stream.Collectors;
import org.rocksdb.CompactionJobInfo;
import org.rocksdb.CompactionOptions;
import org.rocksdb.RocksDB;
import org.rocksdb.RocksDB.LiveFiles;
import org.rocksdb.RocksDBException;

public class RocksLoad {

    public static final int KEY_SIZE = 9;
    public static final int VALUE_SIZE = 1024;
    private static final int NR_VALUES = 1024;
    public static final byte[][] RANDOM_VALUES = new byte[NR_VALUES][VALUE_SIZE];
    private static final Random random = new Random();
    public static final int WRITES_PER_BATCH = 1_000_000;

    static {
        for (int i = 0; i < NR_VALUES; i++) {
            random.nextBytes(RANDOM_VALUES[i]);
        }
    }

    static long totalWrites = 0;

    public String getGreeting() {
        return "Hello world.";
    }

    public static void fillDb(final RocksDB db, final int numBatches, final int numOverwrites) {
        System.out.println("generating DB with number overwrites=" + numOverwrites);

        for (int x = 0; x < numOverwrites; x++) {
            System.out.println("Starting overwrite=" + x);

            for (int i = 0; i < numBatches; i++) {
                System.out.println("Starting batch=" + i);

                try {
                    doWrites(db);
                    System.out.println("Live files after write:");
                    printLiveFiles(db);
                } catch (RocksDBException e) {
                    System.out.println("Error filling DB");
                    e.printStackTrace();
                }
                System.out.println("--------------------------- " + i);
            }

            totalWrites = 0;

        }
    }

    public static void doFullCompactionOnDefaultColumnFamily(RocksDB db, int subcompactions)
            throws RocksDBException {

        long initCompaction = System.nanoTime();
        CompactionOptions compactionOptions2 = new CompactionOptions();
        compactionOptions2.setMaxSubcompactions(0);

        List<String> files = db
                .getColumnFamilyMetaData(db.getDefaultColumnFamily())
                .levels()
                .stream().flatMap(level -> level.files().stream())
                .map(sst -> sst.fileName())
                .collect(Collectors.toList());

        System.out.println("start compaction... sstables_nr=" + files.size());

        CompactionOptions compactionOptions = new CompactionOptions();
        compactionOptions.setMaxSubcompactions(subcompactions);
        compactionOptions.setOutputFileSizeLimit(128_000_000);

        CompactionJobInfo jobInfo = new CompactionJobInfo();

        db.compactFiles(compactionOptions, files, 0, -1, jobInfo);
        long totalCompactionMs = (System.nanoTime() - initCompaction) / 1_000_000;
        System.out.println("total compaction time ms=" + totalCompactionMs + " total files nr=" + db
                .getColumnFamilyMetaData(db.getDefaultColumnFamily()).fileCount());
        System.out.println(jobInfo);
    }

    public static void printLiveFiles(RocksDB db) throws RocksDBException {
        LiveFiles liveFiles = db.getLiveFiles(false);
        System.out.println("live files nr=" + liveFiles.files.size());
        System.out.println(liveFiles.files.stream().collect(Collectors.joining(",")));
    }

    public static void doReads(RocksDB db) throws RocksDBException {
        System.out.println("start reading... ");
        long startReadNs = System.nanoTime();
        for (long i = 0; i < 1_000_000; i++) {
            doRandomRead(db);
        }
        long endReadNs = System.nanoTime();
        long readMs = (endReadNs - startReadNs) / 1_000_000;
        System.out.println("done reading... ms=" + readMs);
    }

    public static byte[] doRandomRead(RocksDB db) throws RocksDBException {
        return db.get(getRandomExistingKey());
    }

    public static void doWrites(RocksDB db) throws RocksDBException {
        System.out.println("start writing... ");
        long startWriteNs = System.nanoTime();

        long i = totalWrites + WRITES_PER_BATCH;
        for (; i >= totalWrites; --i) { // trying to not do inserts sorted.
            db.put(getKey(i), getRandomValue());
        }
        long endWriteNs = System.nanoTime();
        long writeMs = (endWriteNs - startWriteNs) / 1_000_000;
        totalWrites = totalWrites + WRITES_PER_BATCH;
        System.out.println(
                String.format("done writing... ms=%d written range keys=(%d..%d)", writeMs,
                        totalWrites - WRITES_PER_BATCH, totalWrites));
    }

    private static byte[] getRandomValue() {
        // we get values randomly, trying to not favour compression with some repetitive sequence.
        return RANDOM_VALUES[random.nextInt(NR_VALUES)];
    }

    private static byte[] getRandomExistingKey() {
        return getKey(random.nextLong() % 1_000_000);
//        return getKey(random.nextLong() % totalWrites);
    }

    private static byte[] getRandomKeyFromRange(long biggerKey) {
        // get a random key from range of keys: 0..biggerKey
        return getKey(random.nextLong() % biggerKey);
    }

    private static byte[] getKey(long i) {
        byte[] nineBytes = new byte[KEY_SIZE];
        System.arraycopy(Longs.toByteArray(i), 0, nineBytes, 1, 8);
        return nineBytes;
    }
}
